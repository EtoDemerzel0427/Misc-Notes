# Hash tables

**Feature**ï¼š supports the INSERT, DELETE, and FIND operations in expected $O(1)$ time.

**Core idea**: use a *hash function* that maps a **large** keyspace to a **smaller** domain of array indices, and then use constant-time array operations to store and retrieve the data.

**Applications in data structure**:

* **Dictionary** data type(keys and values)
*  **set**(only keys, no values)

**Resolving collisions**:

1. **chaining**: each location a pointer to a linked list. (Consider the worst case for searching in this scenario) 

   **load factor** $\alpha = \dfrac{n}{m}$, average failed search: $O(\alpha$).

2. **Open addressing**: if not in right place, fix some probe sequence, e.g, $H(x), H(x) + 1, H(x)+2, ...$

   $\alpha = \dfrac{n}{m} \leq 1$, or we will run out of space. **In fact we must ensure it is strictly less than 1, or some search may never terminate.**(in this case never will an empty location be reached) The worst case expected cost (probe number) of FIND is bounded by $\dfrac{1}{1 - \dfrac{n}{m}}$.

**Choose a hash function**:

**We want our hash function to look as close as it can to a random function**.

1. **division method**: If keys are large integers, we just compute the remainder mod m, and the m is typically chosen to be a large prime. If we want to hash strings $a_1a_2a_3...a_k$, we can represent it as $\sum_i a_ib^i$, where $b$ is a base chosen to be **larger than the number of characters**. (Typically we don't really compute such a large number, instead, we compute its remainder mode m; but note that **computing remainders is a relatively slow operation**)
2.  **multiplication method**: Very similar to the division method discussed above, but change the base to a small prime, and the modulus m to be something like $2^{32}$.
3. **Universal hashing**:

